---
# Kubernetes version to install
kubernetes_version: v1.30.2

# SSH access configuration
ansible_user: ansibleuser
systemd_dir: /etc/systemd/system

# System configuration
system_timezone: Europe/Amsterdam

# Network configuration
network_interface: eth0

# Container Network Interface (CNI) configuration
# Choose one of: flannel, calico, or cilium
cni_plugin: calico  # Using Calico for better network policies

# Calico CNI configuration (when cni_plugin: calico)
calico_network_backend: bird  # Options: bird, vxlan
calico_ipv4pool_cidr: 10.52.0.0/16
calico_felix_chaininsertmode: Append
calico_version: v3.28.0

# Cilium CNI configuration (when cni_plugin: cilium)
cilium_version: v1.16.0
cilium_tunnel_mode: vxlan    # Options: vxlan, geneve, disabled
cilium_kube_proxy_replacement: strict  # Options: strict, probe, partial, disabled
cilium_enable_hubble: true
cilium_enable_bpf: true

# Cluster networking
cluster_cidr: 10.52.0.0/16
service_cidr: 10.96.0.0/12

# High Availability and Load Balancer Configuration

# Kube-vip Configuration for Control Plane HA
ha_enabled: true
kube_vip_enabled: true
kube_vip_version: v0.8.2  # Version for both HA and LoadBalancer
kube_vip_interface: "{{ network_interface }}"
kube_vip_address: 192.168.30.222  # Virtual IP for the control plane

# Load Balancer Selection
load_balancer_type: kube-vip  # Using kube-vip for predictable IPs

# MetalLB Configuration (when load_balancer_type: metallb)
metallb_version: v0.14.8
metallb_mode: layer2  # Options: layer2, bgp
metallb_address_pool: 192.168.30.80-192.168.30.90

# MetalLB BGP Configuration (when metallb_mode: bgp)
metallb_bgp_enabled: false

# CMS-specific kube-vip ranges
kube_vip_service_ranges:
  frontend: "192.168.30.150-192.168.30.155"  # CMS frontend services
  admin: "192.168.30.156-192.168.30.160"     # Admin interfaces

# CMS-specific resource requirements
control_plane_memory: "4096M"  # 4GB sufficient for control plane
worker_memory: "8192M"         # 8GB for workers
control_plane_cpu: "2"         # 2 CPUs for control plane
worker_cpu: "4"               # 4 CPUs for workers
metallb_bgp_my_asn: "64513"
metallb_bgp_peer_asn: "64512"
metallb_bgp_peer_address: "192.168.30.1"

# Kube-vip LoadBalancer Configuration (when load_balancer_type: kube-vip)
kube_vip_address_pool: "192.168.30.80-192.168.30.90"

# Additional Features
enable_metrics_server: true
metrics_server_version: v0.7.0
enable_dashboard: false
dashboard_version: v3.0.0

# Kubernetes control plane configuration
control_plane_endpoint: "{{ apiserver_endpoint }}:6443"
kubeadm_token: some-SUPER-DEDEUPER-secret-password  # This token will be used for node joining

# Node configuration
node_ip: "{{ ansible_facts[network_interface]['ipv4']['address'] }}"
control_plane_taint: "{{ true if groups['worker'] | default([]) | length >= 1 else false }}"

# Kubeadm configuration
kubeadm_init_config: |
  apiVersion: kubeadm.k8s.io/v1beta3
  kind: InitConfiguration
  nodeRegistration:
    name: "{{ inventory_hostname }}"
    criSocket: "unix:///var/run/containerd/containerd.sock"
    taints:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
        value: ""

kubeadm_cluster_config: |
  apiVersion: kubeadm.k8s.io/v1beta3
  kind: ClusterConfiguration
  kubernetesVersion: "{{ kubernetes_version }}"
  controlPlaneEndpoint: "{{ control_plane_endpoint }}"
  networking:
    podSubnet: "{{ calico_ipv4pool_cidr if cni_plugin == 'calico' else cluster_cidr }}"
    serviceSubnet: "{{ service_cidr }}"
  apiServer:
    extraArgs:
      authorization-mode: Node,RBAC
  controllerManager:
    extraArgs:
      bind-address: 0.0.0.0
  scheduler:
    extraArgs:
      bind-address: 0.0.0.0
  etcd:
    local:
      extraArgs:
        listen-metrics-urls: http://0.0.0.0:2381



# Proxmox LXC Configuration
# IMPORTANT: Read this documentation before using Proxmox LXC containers:
# https://gist.github.com/triangletodd/02f595cd4c0dc9aac5f7763ca2264185
#
# Key requirements for Proxmox LXC containers with Kubernetes:
# 1. Containers MUST be privileged
# 2. Nesting MUST be disabled (nesting: 0)
# 3. Container security features will be modified to support Kubernetes
# 4. Resource efficiency: LXC containers use significantly less resources than VMs
#
# Note: Mixing VMs and LXC containers in the same cluster is not recommended.
# Only use LXC if you need the resource efficiency (e.g., low-powered Proxmox nodes)

proxmox_enabled: false
proxmox_user: root

# Container IDs and IP addresses for Proxmox LXC nodes
proxmox_containers:
  control_plane:
    - id: 200
      ip: 192.168.30.38
      cpu: 2       # Number of CPU cores
      memory: 2048 # Memory in MB
    - id: 201
      ip: 192.168.30.39
      cpu: 2
      memory: 2048
    - id: 202
      ip: 192.168.30.40
      cpu: 2
      memory: 2048
  worker:
    - id: 203
      ip: 192.168.30.41
      cpu: 4
      memory: 4096
    - id: 204
      ip: 192.168.30.42
      cpu: 4
      memory: 4096

# Container Registry Configuration
# Enable if using private registry for caching or air-gapped environments
registry_enabled: false

# Registry configuration example
registry_config: |
  mirrors:
    docker.io:
      endpoint:
        - "https://registry.example.com/v2/docker"
    quay.io:
      endpoint:
        - "https://registry.example.com/v2/quay"
    ghcr.io:
      endpoint:
        - "https://registry.example.com/v2/ghcr"
    registry.example.com:
      endpoint:
        - "https://registry.example.com"

  configs:
    "registry.example.com":
      auth:
        username: "your-username"
        password: "your-password"

# System Configuration

# Custom reboot command (if dbus is not available)
# custom_reboot_command: /usr/sbin/shutdown -r now

# Proxy Configuration (if required)
# proxy_settings:
#   http_proxy: "http://proxy.example.com:3128"
#   https_proxy: "http://proxy.example.com:3128"
#   no_proxy: "localhost,127.0.0.1,.example.com,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16"
